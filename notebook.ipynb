{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855bdaae-d2a3-442c-8943-975fc4f05fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e0cbce-0695-4894-b6db-06edcdb51fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chuncks  95\n"
     ]
    }
   ],
   "source": [
    "file_name = 'state_of_the_union.txt'\n",
    "max_num_of_tokens = 2048\n",
    "loader = TextLoader(file_name)\n",
    "query = \"What did the president say about Ketanji Brown Jackson?\"\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100, separator='\\n')\n",
    "splited_docs = text_splitter.split_documents(documents)\n",
    "print('Number of chuncks ', len(splited_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7777cc1a-30a5-454c-8058-af6033736892",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./gpt4all-lora-quantized-ggml.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = ggmf v1 (old version with no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 1024\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113744.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  = 1024.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "llama.cpp: loading model from ./gpt4all-lora-quantized-ggml.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = ggmf v1 (old version with no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113744.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "llama_init_from_file: kv self size  =  512.00 MB\n"
     ]
    }
   ],
   "source": [
    "model = \"./gpt4all-lora-quantized-ggml.bin\"\n",
    "llm = LlamaCpp(model_path = model, n_ctx=max_num_of_tokens)\n",
    "llm_embeddings = LlamaCppEmbeddings(model_path = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92dd319-4b92-4f84-8edd-03e89fe2164a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db_state_of_the_union.txt\n"
     ]
    }
   ],
   "source": [
    "persist_directory = 'db_' + file_name\n",
    "db = None\n",
    "\n",
    "if os.path.isdir(persist_directory):\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=llm_embeddings)\n",
    "else:\n",
    "    db = Chroma.from_documents(splited_docs, llm_embeddings, persist_directory=persist_directory)\n",
    "    db.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2ea3b-12db-4977-8096-74ff61e1d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opensearch\n",
    "#docsearch = OpenSearchVectorSearch.from_documents(docs, llm_embeddings, opensearch_url=\"http://localhost:9200\")\n",
    "#docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21527a82-09f5-4312-b98b-76bfc3c3222a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   650.47 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1062.61 ms /    14 tokens (   75.90 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1063.99 ms\n"
     ]
    }
   ],
   "source": [
    "# Chroma\n",
    "response_docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "331541ff-f65f-482d-8ae5-470703f4ad8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.', metadata={'source': '/private/var/folders/vq/sbkvnrvx5g71hm826hr91cw00000gn/T/26fa01dd4b62b09f4f48b523716bdd846615ba86/state_of_the_union.txt'}),\n",
       " Document(page_content='Second – we must prepare for new variants. Over the past year, we’ve gotten much better at detecting new variants. \\n\\nIf necessary, we’ll be able to deploy new vaccines within 100 days instead of many more months or years.  \\n\\nAnd, if Congress provides the funds we need, we’ll have new stockpiles of tests, masks, and pills ready if needed. \\n\\nI cannot promise a new variant won’t come. But I can promise you we’ll do everything within our power to be ready if it does.', metadata={'source': '/private/var/folders/vq/sbkvnrvx5g71hm826hr91cw00000gn/T/26fa01dd4b62b09f4f48b523716bdd846615ba86/state_of_the_union.txt'}),\n",
       " Document(page_content='and the middle out, not from the top down.  \\n\\nBecause we know that when the middle class grows, the poor have a ladder up and the wealthy do very well. \\n\\nAmerica used to have the best roads, bridges, and airports on Earth. \\n\\nNow our infrastructure is ranked 13th in the world. \\n\\nWe won’t be able to compete for the jobs of the 21st Century if we don’t fix that. \\n\\nThat’s why it was so important to pass the Bipartisan Infrastructure Law—the most sweeping investment to rebuild America in history.', metadata={'source': '/private/var/folders/vq/sbkvnrvx5g71hm826hr91cw00000gn/T/26fa01dd4b62b09f4f48b523716bdd846615ba86/state_of_the_union.txt'}),\n",
       " Document(page_content='Danielle says Heath was a fighter to the very end. \\n\\nHe didn’t know how to stop fighting, and neither did she. \\n\\nThrough her pain she found purpose to demand we do better. \\n\\nTonight, Danielle—we are. \\n\\nThe VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits. \\n\\nAnd tonight, I’m announcing we’re expanding eligibility to veterans suffering from nine respiratory cancers.', metadata={'source': '/private/var/folders/vq/sbkvnrvx5g71hm826hr91cw00000gn/T/26fa01dd4b62b09f4f48b523716bdd846615ba86/state_of_the_union.txt'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#small_docs = [Document(page_content=response_docs[0].page_content, metadata=response_docs[0].metadata)]\n",
    "response_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d58f359-a1c4-44f7-bdf1-6a0fe6986803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   613.01 ms\n",
      "llama_print_timings:      sample time =    13.13 ms /    18 runs   (    0.73 ms per run)\n",
      "llama_print_timings: prompt eval time = 12175.51 ms /   184 tokens (   66.17 ms per token)\n",
      "llama_print_timings:        eval time =  1345.36 ms /    17 runs   (   79.14 ms per run)\n",
      "llama_print_timings:       total time = 13536.08 ms\n",
      "\n",
      "llama_print_timings:        load time =   613.01 ms\n",
      "llama_print_timings:      sample time =    52.77 ms /    74 runs   (    0.71 ms per run)\n",
      "llama_print_timings: prompt eval time = 12591.76 ms /   192 tokens (   65.58 ms per token)\n",
      "llama_print_timings:        eval time =  5967.13 ms /    74 runs   (   80.64 ms per run)\n",
      "llama_print_timings:       total time = 18617.97 ms\n",
      "\n",
      "llama_print_timings:        load time =   613.01 ms\n",
      "llama_print_timings:      sample time =    22.93 ms /    32 runs   (    0.72 ms per run)\n",
      "llama_print_timings: prompt eval time = 13763.54 ms /   208 tokens (   66.17 ms per token)\n",
      "llama_print_timings:        eval time =  2602.49 ms /    32 runs   (   81.33 ms per run)\n",
      "llama_print_timings:       total time = 16391.96 ms\n",
      "\n",
      "llama_print_timings:        load time =   613.01 ms\n",
      "llama_print_timings:      sample time =    40.74 ms /    56 runs   (    0.73 ms per run)\n",
      "llama_print_timings: prompt eval time = 12708.94 ms /   190 tokens (   66.89 ms per token)\n",
      "llama_print_timings:        eval time =  4845.83 ms /    55 runs   (   88.11 ms per run)\n",
      "llama_print_timings:       total time = 17600.53 ms\n"
     ]
    }
   ],
   "source": [
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "responses = []\n",
    "for rdoc in response_docs:\n",
    "    responses.append(Document(page_content=chain.run(input_documents=[rdoc], question=query),  metadata=rdoc.metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626d5961-a221-49bc-973b-98ef838f4ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   613.01 ms\n",
      "llama_print_timings:      sample time =    39.68 ms /    56 runs   (    0.71 ms per run)\n",
      "llama_print_timings: prompt eval time = 27235.14 ms /   405 tokens (   67.25 ms per token)\n",
      "llama_print_timings:        eval time =  4898.93 ms /    55 runs   (   89.07 ms per run)\n",
      "llama_print_timings:       total time = 32179.77 ms\n"
     ]
    }
   ],
   "source": [
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "while len(responses) != 1:\n",
    "    c = 0\n",
    "    length_contenxt = 0\n",
    "    for i in range(len(responses)):\n",
    "        l = 40 + len(query.split(' ')) + length_contenxt + len(responses[i].page_content.split(' '))\n",
    "        if 1024 > l:\n",
    "            length_contenxt += len(responses[i].page_content.split(' '))\n",
    "            c += 1\n",
    "    \n",
    "    responses.append(Document(page_content=chain.run(input_documents=responses[:c], question=query)))\n",
    "    responses = responses[c:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1adeb0a-f1f0-4e98-86e6-dc30629f1ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=' The president mentioned Ketanji Brown Jackson in his speech regarding the expansion of eligibility for veterans with nine respiratory cancers. He also said that she was an attorney, a judge, and a fierce advocate for veterans’ issues.', metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83a398-09bf-4a6b-afb3-80a818aed3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
